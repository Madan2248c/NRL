<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Q-Learning Intelligent Tutoring System - Project Documentation</title>
    <style>
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --accent-color: #3b82f6;
            --success-color: #10b981;
            --warning-color: #f59e0b;
            --danger-color: #ef4444;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-color: #1e293b;
            --text-muted: #64748b;
            --border-color: #e2e8f0;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background: var(--bg-color);
        }
        
        /* Header */
        .header {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            padding: 60px 20px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 15px;
            font-weight: 700;
        }
        
        .header .subtitle {
            font-size: 1.2rem;
            opacity: 0.9;
            max-width: 700px;
            margin: 0 auto;
        }
        
        .header .badge {
            display: inline-block;
            background: rgba(255,255,255,0.2);
            padding: 8px 20px;
            border-radius: 25px;
            margin-top: 20px;
            font-size: 0.9rem;
        }
        
        /* Navigation */
        .nav {
            background: var(--card-bg);
            padding: 15px 20px;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 10px;
        }
        
        .nav a {
            color: var(--text-color);
            text-decoration: none;
            padding: 8px 16px;
            border-radius: 8px;
            font-size: 0.9rem;
            transition: all 0.3s;
        }
        
        .nav a:hover {
            background: var(--primary-color);
            color: white;
        }
        
        /* Main Content */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        
        /* Sections */
        section {
            background: var(--card-bg);
            border-radius: 16px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.05);
        }
        
        section h2 {
            color: var(--primary-color);
            font-size: 1.8rem;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 3px solid var(--accent-color);
            display: flex;
            align-items: center;
            gap: 12px;
        }
        
        section h3 {
            color: var(--secondary-color);
            font-size: 1.3rem;
            margin: 30px 0 15px;
        }
        
        section h4 {
            color: var(--text-color);
            font-size: 1.1rem;
            margin: 20px 0 10px;
        }
        
        p {
            margin-bottom: 15px;
            color: var(--text-color);
        }
        
        /* Cards Grid */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .card {
            background: linear-gradient(145deg, #f8fafc, #ffffff);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 25px;
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }
        
        .card h4 {
            color: var(--primary-color);
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .card p {
            font-size: 0.95rem;
            color: var(--text-muted);
        }
        
        /* Code Blocks */
        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9rem;
            line-height: 1.5;
        }
        
        .code-block .comment {
            color: #64748b;
        }
        
        .code-block .keyword {
            color: #f472b6;
        }
        
        .code-block .string {
            color: #a5f3fc;
        }
        
        .code-block .number {
            color: #fcd34d;
        }
        
        .code-block .function {
            color: #93c5fd;
        }
        
        code {
            background: #e2e8f0;
            padding: 2px 8px;
            border-radius: 4px;
            font-family: 'Consolas', monospace;
            font-size: 0.9em;
            color: var(--secondary-color);
        }
        
        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95rem;
        }
        
        th, td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }
        
        th {
            background: var(--primary-color);
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: #f1f5f9;
        }
        
        /* Lists */
        ul, ol {
            margin: 15px 0 15px 25px;
        }
        
        li {
            margin-bottom: 10px;
        }
        
        /* Highlight Boxes */
        .highlight-box {
            padding: 20px 25px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .highlight-box.info {
            background: #eff6ff;
            border-left: 4px solid var(--primary-color);
        }
        
        .highlight-box.success {
            background: #ecfdf5;
            border-left: 4px solid var(--success-color);
        }
        
        .highlight-box.warning {
            background: #fffbeb;
            border-left: 4px solid var(--warning-color);
        }
        
        .highlight-box.formula {
            background: #f8fafc;
            border: 2px solid var(--accent-color);
            text-align: center;
            font-size: 1.1rem;
        }
        
        /* Flow Diagram */
        .flow-diagram {
            display: flex;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 30px;
            background: linear-gradient(135deg, #eff6ff, #f8fafc);
            border-radius: 12px;
        }
        
        .flow-box {
            background: white;
            border: 2px solid var(--primary-color);
            border-radius: 10px;
            padding: 15px 25px;
            text-align: center;
            min-width: 150px;
            box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        }
        
        .flow-box h5 {
            color: var(--primary-color);
            margin-bottom: 5px;
        }
        
        .flow-box p {
            font-size: 0.85rem;
            color: var(--text-muted);
            margin: 0;
        }
        
        .flow-arrow {
            font-size: 1.5rem;
            color: var(--primary-color);
        }
        
        /* State Diagram */
        .state-representation {
            background: linear-gradient(135deg, #1e40af, #3b82f6);
            color: white;
            padding: 30px;
            border-radius: 12px;
            margin: 20px 0;
        }
        
        .state-tuple {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 10px;
            margin-top: 15px;
        }
        
        .state-element {
            background: rgba(255,255,255,0.2);
            padding: 12px 20px;
            border-radius: 8px;
            text-align: center;
        }
        
        .state-element .label {
            font-size: 0.75rem;
            opacity: 0.8;
            display: block;
        }
        
        .state-element .value {
            font-size: 1.1rem;
            font-weight: 600;
        }
        
        /* Reward Table Styling */
        .reward-positive {
            color: var(--success-color);
            font-weight: 600;
        }
        
        .reward-negative {
            color: var(--danger-color);
            font-weight: 600;
        }
        
        /* Architecture Diagram */
        .architecture {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .arch-component {
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 12px;
            padding: 25px;
            text-align: center;
        }
        
        .arch-component.env {
            border-color: #0ea5e9;
            background: linear-gradient(135deg, #f0f9ff, white);
        }
        
        .arch-component.agent {
            border-color: #8b5cf6;
            background: linear-gradient(135deg, #f5f3ff, white);
        }
        
        .arch-component.training {
            border-color: #10b981;
            background: linear-gradient(135deg, #ecfdf5, white);
        }
        
        .arch-component.config {
            border-color: #f59e0b;
            background: linear-gradient(135deg, #fffbeb, white);
        }
        
        .arch-component h4 {
            margin-bottom: 10px;
        }
        
        .arch-component .file-name {
            font-family: monospace;
            font-size: 0.85rem;
            color: var(--text-muted);
            margin-bottom: 10px;
        }
        
        /* Q-Learning Update Animation */
        .update-visualization {
            background: #1e293b;
            color: white;
            padding: 30px;
            border-radius: 12px;
            margin: 20px 0;
        }
        
        .update-step {
            display: flex;
            align-items: center;
            gap: 15px;
            margin: 15px 0;
            padding: 15px;
            background: rgba(255,255,255,0.1);
            border-radius: 8px;
        }
        
        .step-number {
            background: var(--primary-color);
            color: white;
            width: 35px;
            height: 35px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            flex-shrink: 0;
        }
        
        /* Inference Section */
        .inference-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .inference-card {
            background: linear-gradient(145deg, #ffffff, #f8fafc);
            border-radius: 12px;
            padding: 25px;
            border: 1px solid var(--border-color);
        }
        
        .inference-card h4 {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-bottom: 15px;
        }
        
        .inference-card .icon {
            font-size: 1.5rem;
        }
        
        /* Footer */
        .footer {
            background: var(--secondary-color);
            color: white;
            padding: 40px 20px;
            text-align: center;
        }
        
        .footer p {
            color: rgba(255,255,255,0.8);
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            
            section {
                padding: 25px;
            }
            
            .flow-diagram {
                flex-direction: column;
            }
            
            .flow-arrow {
                transform: rotate(90deg);
            }
        }
        
        /* Print Styles */
        @media print {
            .nav {
                display: none;
            }
            
            section {
                break-inside: avoid;
            }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <h1>üéì Q-Learning Intelligent Tutoring System</h1>
        <p class="subtitle">A Reinforcement Learning based adaptive tutoring system that learns optimal teaching strategies to maximize student learning outcomes and engagement</p>
        <span class="badge">üìö Machine Learning ‚Ä¢ Reinforcement Learning ‚Ä¢ Q-Learning</span>
    </header>
    
    <!-- Navigation -->
    <nav class="nav">
        <div class="nav-content">
            <a href="#overview">Overview</a>
            <a href="#problem">Problem Statement</a>
            <a href="#architecture">Architecture</a>
            <a href="#rl-concepts">RL Concepts</a>
            <a href="#implementation">Implementation</a>
            <a href="#q-learning">Q-Learning Algorithm</a>
            <a href="#training">Training Process</a>
            <a href="#inference">Key Inferences</a>
            <a href="#run">How to Run</a>
        </div>
    </nav>
    
    <div class="container">
        <!-- Overview Section -->
        <section id="overview">
            <h2>üìã Project Overview</h2>
            <p>This project implements an <strong>Intelligent Tutoring System (ITS)</strong> powered by <strong>Reinforcement Learning (RL)</strong>. The system acts as an AI tutor that learns to make optimal teaching decisions based on student performance and engagement levels.</p>
            
            <div class="highlight-box info">
                <strong>üí° Key Insight:</strong> Traditional tutoring systems follow fixed rules. Our system <em>learns</em> the best teaching strategies through trial and error, adapting to different student states dynamically.
            </div>
            
            <h3>What Does the System Learn?</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>üìù Question Selection</h4>
                    <p>When to present easy, medium, or hard questions based on student's current knowledge level</p>
                </div>
                <div class="card">
                    <h4>üí° Hint Timing</h4>
                    <p>Optimal moments to provide hints without making the student overly dependent</p>
                </div>
                <div class="card">
                    <h4>üìö Topic Navigation</h4>
                    <p>When to advance to next topics or review previous material based on mastery</p>
                </div>
                <div class="card">
                    <h4>üéØ Session Management</h4>
                    <p>Ideal time to end learning sessions for maximum retention and engagement</p>
                </div>
            </div>
        </section>
        
        <!-- Problem Statement -->
        <section id="problem">
            <h2>üéØ Problem Statement</h2>
            <p>Design an intelligent tutoring agent that can:</p>
            <ol>
                <li><strong>Adapt difficulty dynamically</strong> - Present questions matching student's evolving knowledge level</li>
                <li><strong>Prevent student dropout</strong> - Avoid frustration from too-hard questions or boredom from too-easy ones</li>
                <li><strong>Maximize learning outcomes</strong> - Help students progress through knowledge levels efficiently</li>
                <li><strong>Maintain engagement</strong> - Keep students motivated throughout the learning session</li>
            </ol>
            
            <h3>Why Reinforcement Learning?</h3>
            <div class="highlight-box success">
                <p><strong>RL is ideal because:</strong></p>
                <ul>
                    <li>The problem involves <strong>sequential decision making</strong> - each action affects future states</li>
                    <li>Rewards are <strong>delayed</strong> - true learning success is only known at session end</li>
                    <li>State transitions are <strong>stochastic</strong> - student responses are probabilistic</li>
                    <li>We can <strong>simulate</strong> student behavior for training without real students</li>
                </ul>
            </div>
        </section>
        
        <!-- Architecture Section -->
        <section id="architecture">
            <h2>üèóÔ∏è System Architecture</h2>
            
            <div class="architecture">
                <div class="arch-component config">
                    <h4>‚öôÔ∏è Configuration</h4>
                    <div class="file-name">config.py</div>
                    <p>Hyperparameters, state/action mappings, reward values, probabilities</p>
                </div>
                <div class="arch-component env">
                    <h4>üåç Environment</h4>
                    <div class="file-name">student_env.py</div>
                    <p>Simulates student learning dynamics, state transitions, and reward calculation</p>
                </div>
                <div class="arch-component agent">
                    <h4>ü§ñ Agent</h4>
                    <div class="file-name">q_agent.py</div>
                    <p>Q-Learning agent with Œµ-greedy policy, Q-table management, and learning updates</p>
                </div>
                <div class="arch-component training">
                    <h4>üéÆ Training</h4>
                    <div class="file-name">train.py</div>
                    <p>Training loop, episode management, evaluation, and model persistence</p>
                </div>
            </div>
            
            <h3>Data Flow</h3>
            <div class="flow-diagram">
                <div class="flow-box">
                    <h5>Environment</h5>
                    <p>Provides state s</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Agent</h5>
                    <p>Selects action a</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Environment</h5>
                    <p>Returns (s', r, done)</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Agent</h5>
                    <p>Updates Q(s,a)</p>
                </div>
            </div>
        </section>
        
        <!-- RL Concepts -->
        <section id="rl-concepts">
            <h2>üß† Reinforcement Learning Concepts</h2>
            
            <h3>The MDP Framework</h3>
            <p>We model the tutoring problem as a <strong>Markov Decision Process (MDP)</strong> with:</p>
            
            <h4>State Space (S) - What the agent observes</h4>
            <div class="state-representation">
                <p style="text-align:center; margin-bottom:15px;">Each state is a 6-dimensional tuple representing the student's current situation:</p>
                <div class="state-tuple">
                    <div class="state-element">
                        <span class="label">Knowledge Level</span>
                        <span class="value">{0,1,2}</span>
                    </div>
                    <div class="state-element">
                        <span class="label">Current Topic</span>
                        <span class="value">{0,1,2}</span>
                    </div>
                    <div class="state-element">
                        <span class="label">Question Difficulty</span>
                        <span class="value">{0,1,2}</span>
                    </div>
                    <div class="state-element">
                        <span class="label">Correct Streak</span>
                        <span class="value">{0-5}</span>
                    </div>
                    <div class="state-element">
                        <span class="label">Wrong Streak</span>
                        <span class="value">{0-3}</span>
                    </div>
                    <div class="state-element">
                        <span class="label">Engagement</span>
                        <span class="value">{0,1,2}</span>
                    </div>
                </div>
                <p style="text-align:center; margin-top:20px; font-size:0.9rem; opacity:0.8;">
                    <strong>Total State Space:</strong> 3 √ó 3 √ó 3 √ó 6 √ó 4 √ó 3 = <strong>1,944</strong> possible states
                </p>
            </div>
            
            <h4>Action Space (A) - What the agent can do</h4>
            <table>
                <thead>
                    <tr>
                        <th>Action Index</th>
                        <th>Action Name</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0</td>
                        <td>Present_Easy_Question</td>
                        <td>Ask a question below student's level</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>Present_Medium_Question</td>
                        <td>Ask a question at student's level</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>Present_Hard_Question</td>
                        <td>Ask a challenging question</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>Give_Hint</td>
                        <td>Provide assistance to the student</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>Review_Previous_Topic</td>
                        <td>Go back to reinforce earlier concepts</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>Move_To_Next_Topic</td>
                        <td>Advance to new material</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>End_Session</td>
                        <td>Conclude the learning session</td>
                    </tr>
                </tbody>
            </table>
            
            <h4>Reward Function (R) - Teaching the agent what's good/bad</h4>
            <div class="highlight-box warning">
                <strong>üîë Critical Design Decision:</strong> The reward function encodes our teaching philosophy. It defines what behaviors we want the agent to learn.
            </div>
            
            <table>
                <thead>
                    <tr>
                        <th>Event</th>
                        <th>Reward</th>
                        <th>Rationale</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Correct answer (Hard question)</td>
                        <td class="reward-positive">+10</td>
                        <td>Reward challenging the student appropriately</td>
                    </tr>
                    <tr>
                        <td>Correct answer (Medium question)</td>
                        <td class="reward-positive">+5</td>
                        <td>Good progress at appropriate level</td>
                    </tr>
                    <tr>
                        <td>Correct answer (Easy question)</td>
                        <td class="reward-positive">+3</td>
                        <td>Small reward - may be too easy</td>
                    </tr>
                    <tr>
                        <td>Knowledge Level Up</td>
                        <td class="reward-positive">+20</td>
                        <td>Major milestone - student improved!</td>
                    </tr>
                    <tr>
                        <td>Topic Completion</td>
                        <td class="reward-positive">+15</td>
                        <td>Student mastered a topic</td>
                    </tr>
                    <tr>
                        <td>Engagement Increase</td>
                        <td class="reward-positive">+8</td>
                        <td>Student is more motivated</td>
                    </tr>
                    <tr>
                        <td>Successful Session End</td>
                        <td class="reward-positive">+50</td>
                        <td>Best outcome - engaged + learned</td>
                    </tr>
                    <tr>
                        <td>Wrong Answer</td>
                        <td class="reward-negative">-5</td>
                        <td>Setback in learning</td>
                    </tr>
                    <tr>
                        <td>Question Too Easy</td>
                        <td class="reward-negative">-8</td>
                        <td>Wasting student's time</td>
                    </tr>
                    <tr>
                        <td>Question Too Hard</td>
                        <td class="reward-negative">-12</td>
                        <td>Frustrating the student</td>
                    </tr>
                    <tr>
                        <td>Student Dropout</td>
                        <td class="reward-negative">-15</td>
                        <td>Student gave up</td>
                    </tr>
                    <tr>
                        <td>Constraint Violation</td>
                        <td class="reward-negative">-20</td>
                        <td>Invalid teaching action</td>
                    </tr>
                    <tr>
                        <td>Early Session Quit</td>
                        <td class="reward-negative">-30</td>
                        <td>Ended without achieving goals</td>
                    </tr>
                </tbody>
            </table>
            
            <h4>Student Response Model (Transition Dynamics)</h4>
            <p>The probability of a correct answer depends on question difficulty vs. knowledge level:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>P(Correct)</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Difficulty &lt; Knowledge</td>
                        <td><strong>80%</strong></td>
                        <td>Easy question for Advanced student</td>
                    </tr>
                    <tr>
                        <td>Difficulty = Knowledge</td>
                        <td><strong>60%</strong></td>
                        <td>Medium question for Intermediate student</td>
                    </tr>
                    <tr>
                        <td>Difficulty &gt; Knowledge</td>
                        <td><strong>30%</strong></td>
                        <td>Hard question for Beginner student</td>
                    </tr>
                </tbody>
            </table>
            
            <h4>Action Constraints</h4>
            <div class="highlight-box info">
                <p><strong>The agent must learn to respect these teaching constraints:</strong></p>
                <ul>
                    <li>‚ùå Cannot give <strong>hard questions</strong> to <strong>beginners</strong> (would frustrate them)</li>
                    <li>‚ùå Cannot <strong>move to next topic</strong> without <strong>3 consecutive correct</strong> answers</li>
                    <li>‚ùå Cannot <strong>review previous topic</strong> when already on <strong>first topic</strong></li>
                </ul>
                <p style="margin-top:10px;"><em>Violations incur a -20 penalty, teaching the agent to avoid invalid actions.</em></p>
            </div>
        </section>
        
        <!-- Q-Learning Algorithm -->
        <section id="q-learning">
            <h2>üìä Q-Learning Algorithm</h2>
            
            <h3>What is Q-Learning?</h3>
            <p>Q-Learning is a <strong>model-free</strong>, <strong>off-policy</strong> reinforcement learning algorithm that learns the value of taking an action in a given state.</p>
            
            <div class="highlight-box formula">
                <strong>Q-Value Update Rule (Bellman Equation):</strong><br><br>
                <code style="font-size:1.2rem; background:#1e293b; color:#e2e8f0; padding:15px 25px; border-radius:8px; display:inline-block;">
                    Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max<sub>a'</sub>Q(s',a') - Q(s,a)]
                </code>
            </div>
            
            <div class="update-visualization">
                <h4 style="margin-bottom:20px; color:#93c5fd;">Understanding the Update Rule:</h4>
                <div class="update-step">
                    <span class="step-number">Œ±</span>
                    <div>
                        <strong>Learning Rate (Œ± = 0.1)</strong><br>
                        <span style="color:#94a3b8;">How much new information overrides old. Small Œ± = slow but stable learning.</span>
                    </div>
                </div>
                <div class="update-step">
                    <span class="step-number">r</span>
                    <div>
                        <strong>Immediate Reward</strong><br>
                        <span style="color:#94a3b8;">The reward received after taking action a in state s.</span>
                    </div>
                </div>
                <div class="update-step">
                    <span class="step-number">Œ≥</span>
                    <div>
                        <strong>Discount Factor (Œ≥ = 0.9)</strong><br>
                        <span style="color:#94a3b8;">Importance of future rewards. Œ≥=0.9 means future rewards are almost as important as immediate ones.</span>
                    </div>
                </div>
                <div class="update-step">
                    <span class="step-number">max</span>
                    <div>
                        <strong>Best Future Value</strong><br>
                        <span style="color:#94a3b8;">max<sub>a'</sub>Q(s',a') - the best Q-value achievable from the next state s'.</span>
                    </div>
                </div>
            </div>
            
            <h3>Exploration vs. Exploitation (Œµ-Greedy Policy)</h3>
            <p>The agent faces a dilemma: should it <strong>explore</strong> new actions or <strong>exploit</strong> known good actions?</p>
            
            <div class="code-block">
<span class="keyword">def</span> <span class="function">choose_action</span>(state):
    <span class="keyword">if</span> random() &lt; Œµ:          <span class="comment"># With probability Œµ</span>
        <span class="keyword">return</span> random_action()   <span class="comment"># Explore: try something new</span>
    <span class="keyword">else</span>:                        <span class="comment"># With probability 1-Œµ</span>
        <span class="keyword">return</span> argmax(Q[state])  <span class="comment"># Exploit: use best known action</span>
            </div>
            
            <h4>Epsilon Decay Schedule</h4>
            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Value</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Œµ<sub>start</sub></td>
                        <td>1.0 (100%)</td>
                        <td>Start with full exploration</td>
                    </tr>
                    <tr>
                        <td>Œµ<sub>decay</sub></td>
                        <td>0.995</td>
                        <td>Multiply Œµ by this each episode</td>
                    </tr>
                    <tr>
                        <td>Œµ<sub>min</sub></td>
                        <td>0.01 (1%)</td>
                        <td>Never stop exploring completely</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="highlight-box success">
                <strong>Result:</strong> After ~460 episodes, Œµ drops from 1.0 to ~0.1. The agent transitions from random exploration to primarily exploiting learned knowledge while still occasionally trying new actions.
            </div>
            
            <h3>Q-Table Data Structure</h3>
            <p>We use a <strong>dictionary-based sparse Q-table</strong> for efficiency:</p>
            
            <div class="code-block">
<span class="comment"># Q-table structure: Dict[State, List[Q-values]]</span>
q_table = {
    (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>): [<span class="number">0.0</span>, <span class="number">5.2</span>, <span class="number">-3.1</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">-2.0</span>, <span class="number">0.0</span>],
    <span class="comment">#  ‚Üë state tuple         ‚Üë Q-values for actions 0-6</span>
    
    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>): [<span class="number">2.1</span>, <span class="number">8.7</span>, <span class="number">6.3</span>, <span class="number">-1.0</span>, <span class="number">-5.0</span>, <span class="number">15.2</span>, <span class="number">10.0</span>],
    <span class="comment"># ... only visited states are stored</span>
}
            </div>
            
            <p><strong>Why sparse storage?</strong> Of 1,944 possible states, only ~300-600 are typically visited during training. This saves memory and speeds up lookups.</p>
        </section>
        
        <!-- Implementation Details -->
        <section id="implementation">
            <h2>üíª Implementation Details</h2>
            
            <h3>File Structure</h3>
            <div class="code-block">
NRL/
‚îú‚îÄ‚îÄ config.py          <span class="comment"># All hyperparameters and constants</span>
‚îú‚îÄ‚îÄ student_env.py     <span class="comment"># Gym-like environment class</span>
‚îú‚îÄ‚îÄ q_agent.py         <span class="comment"># Q-Learning agent implementation</span>
‚îú‚îÄ‚îÄ train.py           <span class="comment"># Training loop and evaluation</span>
‚îú‚îÄ‚îÄ plot_results.py    <span class="comment"># Visualization of results</span>
‚îú‚îÄ‚îÄ trained_agent.pkl  <span class="comment"># Saved Q-table (generated)</span>
‚îî‚îÄ‚îÄ training_rewards.pkl <span class="comment"># Training history (generated)</span>
            </div>
            
            <h3>Key Code Components</h3>
            
            <h4>1. Environment Step Function</h4>
            <div class="code-block">
<span class="keyword">def</span> <span class="function">step</span>(self, action):
    <span class="comment"># Validate action against constraints</span>
    <span class="keyword">if not</span> self._is_action_valid(action):
        <span class="keyword">return</span> self.state, REWARDS[<span class="string">"constraint_violation"</span>], <span class="keyword">False</span>, {<span class="string">"violation"</span>: <span class="keyword">True</span>}
    
    <span class="comment"># Execute action and calculate reward</span>
    reward, done, info = self._execute_action(action)
    
    <span class="comment"># Update state based on student response</span>
    self._update_state(action)
    
    <span class="keyword">return</span> self.state, reward, done, info
            </div>
            
            <h4>2. Q-Learning Update</h4>
            <div class="code-block">
<span class="keyword">def</span> <span class="function">update</span>(self, state, action, reward, next_state):
    <span class="comment"># Current Q-value</span>
    current_q = self.get_q_value(state, action)
    
    <span class="comment"># Maximum Q-value for next state (best future)</span>
    max_next_q = max(self.q_table[next_state])
    
    <span class="comment"># Q-Learning update rule</span>
    new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
    
    self.set_q_value(state, action, new_q)
            </div>
            
            <h4>3. Training Loop</h4>
            <div class="code-block">
<span class="keyword">for</span> episode <span class="keyword">in</span> range(NUM_EPISODES):  <span class="comment"># 1000 episodes</span>
    state = env.reset()
    episode_reward = <span class="number">0</span>
    
    <span class="keyword">while not</span> done <span class="keyword">and</span> steps &lt; MAX_STEPS:  <span class="comment"># Max 30 steps</span>
        action = agent.choose_action(state)       <span class="comment"># Œµ-greedy</span>
        next_state, reward, done, info = env.step(action)
        agent.update(state, action, reward, next_state)  <span class="comment"># Learn!</span>
        
        episode_reward += reward
        state = next_state
        steps += <span class="number">1</span>
    
    agent.decay_epsilon()  <span class="comment"># Reduce exploration over time</span>
            </div>
        </section>
        
        <!-- Training Process -->
        <section id="training">
            <h2>üéØ Training Process</h2>
            
            <h3>Episode Lifecycle</h3>
            <div class="flow-diagram">
                <div class="flow-box">
                    <h5>Initialize</h5>
                    <p>Reset to state (0,0,0,0,0,1)</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Select Action</h5>
                    <p>Œµ-greedy policy</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Execute</h5>
                    <p>Get reward &amp; new state</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Update Q</h5>
                    <p>Learn from experience</p>
                </div>
                <span class="flow-arrow">‚Üí</span>
                <div class="flow-box">
                    <h5>Repeat / End</h5>
                    <p>Until done or 30 steps</p>
                </div>
            </div>
            
            <h3>Training Hyperparameters</h3>
            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Value</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>NUM_EPISODES</td>
                        <td>1000</td>
                        <td>Total training episodes</td>
                    </tr>
                    <tr>
                        <td>MAX_STEPS</td>
                        <td>30</td>
                        <td>Maximum steps per episode</td>
                    </tr>
                    <tr>
                        <td>Œ± (alpha)</td>
                        <td>0.1</td>
                        <td>Learning rate</td>
                    </tr>
                    <tr>
                        <td>Œ≥ (gamma)</td>
                        <td>0.9</td>
                        <td>Discount factor</td>
                    </tr>
                    <tr>
                        <td>Œµ<sub>start</sub></td>
                        <td>1.0</td>
                        <td>Initial exploration rate</td>
                    </tr>
                    <tr>
                        <td>Œµ<sub>decay</sub></td>
                        <td>0.995</td>
                        <td>Epsilon decay per episode</td>
                    </tr>
                    <tr>
                        <td>Œµ<sub>min</sub></td>
                        <td>0.01</td>
                        <td>Minimum exploration rate</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Expected Training Progression</h3>
            <table>
                <thead>
                    <tr>
                        <th>Phase</th>
                        <th>Episodes</th>
                        <th>Epsilon</th>
                        <th>Expected Reward</th>
                        <th>Behavior</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Early</strong></td>
                        <td>1-200</td>
                        <td>1.0 ‚Üí 0.37</td>
                        <td>-50 to -200</td>
                        <td>Random exploration, many constraint violations</td>
                    </tr>
                    <tr>
                        <td><strong>Middle</strong></td>
                        <td>200-500</td>
                        <td>0.37 ‚Üí 0.08</td>
                        <td>-20 to +20</td>
                        <td>Learning valid actions, improving policy</td>
                    </tr>
                    <tr>
                        <td><strong>Late</strong></td>
                        <td>500-1000</td>
                        <td>0.08 ‚Üí 0.01</td>
                        <td>+30 to +80</td>
                        <td>Near-optimal policy, stable performance</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Termination Conditions</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>‚úÖ Successful Session</h4>
                    <p>Agent chooses End_Session with high engagement (2) and ‚â•3 consecutive correct answers. <span class="reward-positive">+50 reward</span></p>
                </div>
                <div class="card">
                    <h4>‚ùå Student Dropout</h4>
                    <p>3 consecutive wrong answers - student becomes frustrated and quits. <span class="reward-negative">-15 reward</span></p>
                </div>
                <div class="card">
                    <h4>‚è±Ô∏è Max Steps Reached</h4>
                    <p>Episode ends after 30 steps if neither success nor dropout occurred.</p>
                </div>
                <div class="card">
                    <h4>üö™ Early Quit</h4>
                    <p>Agent ends session without achieving success criteria. <span class="reward-negative">-30 reward</span></p>
                </div>
            </div>
        </section>
        
        <!-- Key Inferences -->
        <section id="inference">
            <h2>üîç Key Inferences &amp; Results</h2>
            
            <h3>What Can We Learn From This Project?</h3>
            
            <div class="inference-grid">
                <div class="inference-card">
                    <h4><span class="icon">üìà</span> Convergence Behavior</h4>
                    <p>The moving average of rewards shows clear convergence, indicating the agent successfully learns a good policy. Early random behavior gives way to consistent positive rewards.</p>
                </div>
                
                <div class="inference-card">
                    <h4><span class="icon">üéØ</span> Learned Teaching Strategy</h4>
                    <p>The trained agent learns to:</p>
                    <ul>
                        <li>Match question difficulty to student knowledge</li>
                        <li>Avoid constraint violations completely</li>
                        <li>Time topic transitions appropriately</li>
                        <li>Maintain student engagement</li>
                    </ul>
                </div>
                
                <div class="inference-card">
                    <h4><span class="icon">‚öñÔ∏è</span> Exploration-Exploitation Trade-off</h4>
                    <p>The Œµ-decay schedule demonstrates how the agent transitions from exploring the state space to exploiting learned knowledge. This balance is crucial for good performance.</p>
                </div>
                
                <div class="inference-card">
                    <h4><span class="icon">üèÜ</span> Reward Shaping Impact</h4>
                    <p>Dense rewards (for small achievements) guide learning more effectively than sparse rewards alone. The combination of immediate and terminal rewards creates a well-shaped learning signal.</p>
                </div>
                
                <div class="inference-card">
                    <h4><span class="icon">üìä</span> State Space Coverage</h4>
                    <p>Only 300-600 of 1,944 possible states are visited, demonstrating the efficiency of tabular Q-learning with sparse storage for moderately-sized state spaces.</p>
                </div>
                
                <div class="inference-card">
                    <h4><span class="icon">üîÑ</span> Policy Stability</h4>
                    <p>After sufficient training, the policy stabilizes. The agent consistently makes good decisions, showing that Q-learning has found a near-optimal solution for this MDP.</p>
                </div>
            </div>
            
            <h3>Performance Metrics (Expected)</h3>
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Expected Value</th>
                        <th>Interpretation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Success Rate</td>
                        <td>&gt;80%</td>
                        <td>Agent achieves successful session endings most of the time</td>
                    </tr>
                    <tr>
                        <td>Average Episode Reward</td>
                        <td>+40 to +60</td>
                        <td>Positive rewards indicate effective teaching</td>
                    </tr>
                    <tr>
                        <td>States Visited</td>
                        <td>300-600</td>
                        <td>Agent explores relevant portions of state space</td>
                    </tr>
                    <tr>
                        <td>Dropout Rate</td>
                        <td>&lt;10%</td>
                        <td>Rarely frustrates students to the point of quitting</td>
                    </tr>
                    <tr>
                        <td>Level-Up Rate</td>
                        <td>High</td>
                        <td>Students progress through knowledge levels</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Theoretical Insights</h3>
            
            <div class="highlight-box info">
                <h4>Why Does Q-Learning Work Here?</h4>
                <ol>
                    <li><strong>Finite State Space:</strong> With 1,944 states, tabular Q-learning is computationally feasible</li>
                    <li><strong>Markov Property:</strong> Future states depend only on current state and action, not history</li>
                    <li><strong>Exploration Guarantee:</strong> Œµ-greedy ensures all state-action pairs are visited</li>
                    <li><strong>Well-Designed Rewards:</strong> Rewards align with educational objectives</li>
                    <li><strong>Sufficient Training:</strong> 1000 episodes provide enough experience for convergence</li>
                </ol>
            </div>
            
            <h3>Limitations &amp; Future Improvements</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>üìâ Scalability</h4>
                    <p>Tabular Q-learning doesn't scale well to larger state spaces. For more complex environments, Deep Q-Learning (DQN) would be needed.</p>
                </div>
                <div class="card">
                    <h4>üé≠ Student Variability</h4>
                    <p>Current model assumes one student type. Real systems need to handle diverse learning styles and abilities.</p>
                </div>
                <div class="card">
                    <h4>üìö Content Integration</h4>
                    <p>The environment simulates abstract "topics" and "questions." A real system would need actual educational content.</p>
                </div>
                <div class="card">
                    <h4>üîÑ Transfer Learning</h4>
                    <p>Learned policies don't transfer between different subjects. Each domain would require separate training.</p>
                </div>
            </div>
        </section>
        
        <!-- How to Run -->
        <section id="run">
            <h2>üöÄ How to Run the Project</h2>
            
            <h3>Prerequisites</h3>
            <div class="code-block">
<span class="comment"># Required: Python 3.8+</span>
<span class="comment"># Install dependencies:</span>
pip install numpy matplotlib
            </div>
            
            <h3>Step 1: Train the Agent</h3>
            <div class="code-block">
<span class="comment"># Run training (takes ~30 seconds for 1000 episodes)</span>
python train.py

<span class="comment"># Optional: Customize training</span>
python train.py --episodes <span class="number">500</span> --eval-interval <span class="number">50</span>
            </div>
            
            <h3>Step 2: Visualize Results</h3>
            <div class="code-block">
<span class="comment"># Generate training plots</span>
python plot_results.py

<span class="comment"># This creates:</span>
<span class="comment"># - training_rewards.png (reward progression)</span>
<span class="comment"># - reward_distribution.png (histogram)</span>
<span class="comment"># - learning_curves.png (combined analysis)</span>
            </div>
            
            <h3>Quick Development Cycle</h3>
            <div class="code-block">
<span class="comment"># For fast iteration during development, modify config.py:</span>
NUM_EPISODES = <span class="number">50</span>    <span class="comment"># Instead of 1000</span>
EVAL_INTERVAL = <span class="number">5</span>    <span class="comment"># Instead of 100</span>

<span class="comment"># Then run:</span>
python train.py
            </div>
            
            <h3>Output Files</h3>
            <table>
                <thead>
                    <tr>
                        <th>File</th>
                        <th>Contents</th>
                        <th>Used By</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>trained_agent.pkl</code></td>
                        <td>Pickled Q-table dictionary</td>
                        <td>Evaluation, deployment</td>
                    </tr>
                    <tr>
                        <td><code>training_rewards.pkl</code></td>
                        <td>Episode rewards, lengths, stats</td>
                        <td>plot_results.py</td>
                    </tr>
                    <tr>
                        <td><code>training_rewards.png</code></td>
                        <td>Reward progression plot</td>
                        <td>Analysis</td>
                    </tr>
                    <tr>
                        <td><code>reward_distribution.png</code></td>
                        <td>Reward histogram</td>
                        <td>Analysis</td>
                    </tr>
                    <tr>
                        <td><code>learning_curves.png</code></td>
                        <td>Combined learning curves</td>
                        <td>Analysis</td>
                    </tr>
                </tbody>
            </table>
        </section>
        
        <!-- Summary -->
        <section>
            <h2>üìù Summary</h2>
            
            <div class="highlight-box success">
                <h4>What We Built</h4>
                <p>An intelligent tutoring system that uses <strong>Q-Learning</strong> to learn optimal teaching strategies. The agent learns to balance question difficulty, timing of interventions, and session management to maximize student learning and engagement.</p>
            </div>
            
            <h3>Key Takeaways</h3>
            <ol>
                <li><strong>RL for Education:</strong> Reinforcement learning can effectively model the sequential decision-making required in adaptive tutoring</li>
                <li><strong>Reward Design Matters:</strong> The reward function encodes teaching philosophy and drives what the agent learns</li>
                <li><strong>Exploration is Essential:</strong> Œµ-greedy exploration ensures the agent discovers good strategies</li>
                <li><strong>Tabular Methods Work:</strong> For moderate state spaces, simple Q-learning is effective and interpretable</li>
                <li><strong>Constraints as Penalties:</strong> Invalid actions are handled through negative rewards, teaching the agent boundaries</li>
            </ol>
            
            <h3>Technical Skills Demonstrated</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>üß† Machine Learning</h4>
                    <p>Reinforcement Learning, Q-Learning, MDP formulation, reward shaping</p>
                </div>
                <div class="card">
                    <h4>üêç Python Programming</h4>
                    <p>OOP, NumPy, Matplotlib, pickle serialization, modular design</p>
                </div>
                <div class="card">
                    <h4>üìä Data Analysis</h4>
                    <p>Training visualization, moving averages, performance metrics</p>
                </div>
                <div class="card">
                    <h4>üéÆ Simulation</h4>
                    <p>Environment design, state transitions, probabilistic modeling</p>
                </div>
            </div>
        </section>
    </div>
    
    <!-- Footer -->
    <footer class="footer">
        <p><strong>Q-Learning Intelligent Tutoring System</strong></p>
        <p>Reinforcement Learning Project | Machine Learning</p>
        <p style="margin-top: 15px; font-size: 0.9rem;">Documentation generated for faculty review</p>
    </footer>
</body>
</html>